{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbiTZZmczpi5"
   },
   "source": [
    "# Indonesian Text Classification\n",
    "\n",
    "This is a text classification  of indonesian corpus using several different technique such as Naive Bayes, SVM, Random Forest, Convolition Neural Network (CNN), LSTM or GRU. An indonesian [pre-trained word vectors](https://fasttext.cc/docs/en/pretrained-vectors.html) from FastText has ben also used in our Neural Network models.\n",
    "We use [Word Bahasa Indonesia Corpus and Parallel English Translation](https://www.panl10n.net/english/outputs/Indonesia/BPPT/0902/BPPTIndToEngCorpusHalfM.zip) dataset from PAN Localization.\n",
    "It contains 500,000 words from various online sources translated into English.\n",
    "For our text classification, we use only the indonesian part.\n",
    "The corpus has 4 classes:\n",
    "  - 0: Economy\n",
    "  - 1: International\n",
    "  - 2: Science\n",
    "  - 3: Sport\n",
    " \n",
    "Originally each class is in separate file, we combine, randomize and split it to train and test file with 90:10.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/cahya/Work/miniconda3/envs/fastai/lib/python3.6/site-packages (0.80)\n",
      "Requirement already satisfied: numpy in /home/cahya/Work/miniconda3/envs/fastai/lib/python3.6/site-packages (from xgboost) (1.14.5)\n",
      "Requirement already satisfied: scipy in /home/cahya/Work/miniconda3/envs/fastai/lib/python3.6/site-packages (from xgboost) (1.1.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aUvM1cMBuWq7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn import linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import tensorflow as tf\n",
    "#import pandas as pd, xgboost, numpy, textblob, string\n",
    "import pandas as pd, xgboost, numpy, string\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "#import ntlk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnhzNIbNTS63"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#LMDATA = Path('/content/drive/My Drive/lmdata')\n",
    "LMDATA = Path('/mnt/mldata/data/LM/id/dataset')\n",
    "params = {'batch_size': 1024,\n",
    "          'n_classes': 2,\n",
    "          'max_len': 100,\n",
    "          'n_words': 50000,\n",
    "          'shuffle': True}\n",
    "try:\n",
    "  TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "  strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
    "      tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
    "except KeyError:\n",
    "  TPU_WORKER = None\n",
    "\n",
    "np.random.seed(seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11812,
     "status": "ok",
     "timestamp": 1539291744966,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "-oIkg4wIONNn",
    "outputId": "e063f0a8-feb9-4d86-9b69-2d7dd6ae5024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7432,
     "status": "ok",
     "timestamp": 1539291752437,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "tMk8Dhu4gvv9",
    "outputId": "c0083857-9151-4292-de0e-02c0e0153dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cahya/Work/Machine Learning/LM/language-modeling/indonesia\n",
      "/mnt/mldata/data/LM/id/dataset\n",
      "total 445M\n",
      "drwx------ 2 cahya cahya 4.0K Nov  6 06:56 BPPTIndToEngCorpus\n",
      "-rw-rw-r-- 1 cahya cahya 2.3M Mar 26  2012 BPPTIndToEngCorpusHalfM.zip\n",
      "drwx------ 7 cahya cahya   76 Jan 29  2010 Parallel Corpus\n",
      "-rw-rw-r-- 1 cahya cahya 1.2M Mar 26  2012 Parallel Corpus.zip\n",
      "-rw-rw-r-- 1 cahya cahya 8.4M Oct 27  2009 UI-1M-tagged.txt\n",
      "-rw-rw-r-- 1 cahya cahya 2.3M Mar 26  2012 UI-1M-tagged.zip\n",
      "-rw-rw-r-- 1 cahya cahya  31M Oct 16 11:09 bard.h5\n",
      "drwxrwxr-x 2 cahya cahya   98 Jan 20 19:56 clickbait\n",
      "-rw-rw-r-- 1 cahya cahya  31M Oct 16 11:09 cnn.h5\n",
      "-rw-rw-r-- 1 cahya cahya  35M Oct 16 14:01 cnn_kimyoon.h5\n",
      "-rw-rw-r-- 1 cahya cahya  31M Oct 22 17:49 rcnn.h5\n",
      "-rw-rw-r-- 1 cahya cahya  32M Oct 22 17:46 rnn_bidirectional.h5\n",
      "-rw-rw-r-- 1 cahya cahya  31M Oct 22 17:43 rnn_gru.h5\n",
      "-rw-rw-r-- 1 cahya cahya  32M Oct 22 17:41 rnn_lstm.h5\n",
      "-rw-r--r-- 1 cahya cahya  25M Oct 15 17:30 wiki.id.10K.vec\n",
      "-rw-r--r-- 1 cahya cahya 185M Oct 15 17:32 wiki.id.300K.vec\n",
      "total 816M\n",
      "-rw------- 1 cahya cahya 996K Mar 31  2009 PANL-BPPT-ECO-EN-150Kw.txt\n",
      "-rw------- 1 cahya cahya 1.1M Mar 31  2009 PANL-BPPT-ECO-ID-150Kw.txt\n",
      "-rw------- 1 cahya cahya 973K Mar 31  2009 PANL-BPPT-INT-EN-150Kw.txt\n",
      "-rw------- 1 cahya cahya 1.1M Mar 31  2009 PANL-BPPT-INT-ID-150Kw.txt\n",
      "-rw------- 1 cahya cahya 676K Mar 31  2009 PANL-BPPT-SCI-EN-100Kw.txt\n",
      "-rw------- 1 cahya cahya 734K Mar 31  2009 PANL-BPPT-SCI-ID-100Kw.txt\n",
      "-rw------- 1 cahya cahya 597K Mar 31  2009 PANL-BPPT-SPO-EN-100Kw.txt\n",
      "-rw------- 1 cahya cahya 693K Mar 31  2009 PANL-BPPT-SPO-ID-100Kw.txt\n",
      "-rw-r--r-- 1 cahya cahya 3.6M Jan 20 19:00 bppt_panl.csv\n",
      "-rw-r--r-- 1 cahya cahya 362K Jan 20 19:00 bppt_panl_test.csv\n",
      "-rw-rw-r-- 1 cahya cahya 214K Nov  6 06:51 bppt_panl_test.txt\n",
      "-rw-r--r-- 1 cahya cahya 3.2M Jan 20 19:00 bppt_panl_train.csv\n",
      "-rw-rw-r-- 1 cahya cahya 1.9M Nov  6 06:51 bppt_panl_train.txt\n",
      "-rw-rw-r-- 1 cahya cahya 775M Nov  6 07:03 model_bppt.bin\n",
      "-rw-rw-r-- 1 cahya cahya  26M Nov  6 07:03 model_bppt.vec\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "print(LMDATA)\n",
    "!ls -lh \"$LMDATA\"\n",
    "!ls -lh \"$LMDATA/BPPTIndToEngCorpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7JIIAgpafrB"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_train.csv')\n",
    "train_df.columns = ['label', 'text']\n",
    "test_df = pd.read_csv(LMDATA/'BPPTIndToEngCorpus/bppt_panl_test.csv')\n",
    "test_df.columns = ['label', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301,
     "status": "ok",
     "timestamp": 1539291758045,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "5r4rofDLbCPm",
    "outputId": "d9c0498c-a4f6-4b63-94d1-a26e5e2524dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      0  Pertumbuhan ekonomi 2007 yang diproyeksikan me...\n",
      "1      3  Pelatih Real Bernd Schuster harus mengeluarkan...\n",
      "2      2  Laporan itu adalah pengumuman kedua dari badan...\n",
      "3      0  Lonjakan laba bersih tersebut, selain didorong...\n",
      "4      3  LeBron James menyumbang 24 poin, 11 assist dan...\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())\n",
    "#print(train_df['label'][:10].values)\n",
    "#print(train_df['text'][:10].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5vrNcqhRyXv"
   },
   "outputs": [],
   "source": [
    "!set |grep -i tpu|grep -v grep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nf8wMdASa-d"
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_df['text'], train_df['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1539291787256,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "d9LwD3V_TOJQ",
    "outputId": "595bcbda-674c-4eff-d693-67c0be4abfb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19555    Layanan fantastis ini membutuhkan waktu lama u...\n",
      "9774     Gasparotto memimpin lomba keseluruhan disusul ...\n",
      "10459    Pereli Prancis itu unggul dua menit 33,2 detik...\n",
      "19916    Karena itu Antam juga melakukan diversifikasi ...\n",
      "15343    Para kapitalis ini menginginkan pemerintah Ind...\n",
      "Name: text, dtype: object\n",
      "[2 3 3 0 0]\n",
      "3081     Menurut Shahab, peralatan Rig pada BJP-1R1 sud...\n",
      "1601     Anak pepsis memakan daging tarantula dan berli...\n",
      "20221    Dalam hal kepemilikan asset, maka sektor-sekto...\n",
      "2118     Ilmuwan telah menemukan informasi genetik yang...\n",
      "17469    Anda menyesuaikan dengan itu dan lawan juga sama.\n",
      "Name: text, dtype: object\n",
      "[0 2 0 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[:5])\n",
    "print(train_y[:5])\n",
    "print(valid_x[:5])\n",
    "print(valid_y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMIMaiYGSvaV"
   },
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_df['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8dS7tm2i5e32"
   },
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                             max_features=5000)\n",
    "tfidf_vect.fit(train_df['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                                   ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(train_df['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}',\n",
    "                                         ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(train_df['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31442,
     "status": "ok",
     "timestamp": 1539291844633,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "YErL4IVA5jrD",
    "outputId": "e917deea-159f-40d9-c33a-3d832816b779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50000\n",
      "Values: 73989: ['pembinaannya', '-0.087859', '0.17738', '-0.25976', '0.24112', '-0.26144', '0.10555', '-0.37918', '-0.11241', '-0.15916', '-0.31183', '0.037231', '-0.42092', '0.18129', '-0.12284', '0.11765', '0.09406', '0.26735', '-0.21884', '-0.11559', '-0.22417', '-0.25178', '-0.13234', '-0.17492', '-0.2665', '0.060694', '-0.085088', '0.080088', '0.057243', '0.18208', '-0.29722', '0.28678', '-0.21725', '-0.25867', '-0.40978', '-0.021054', '-0.16561', '0.15877', '-0.276', '0.24313', '-0.31692', '-0.096804', '0.012354', '-0.010719', '0.40314', '0.22857', '-0.089445', '-0.083771', '0.31009', '-0.0004702', '-0.044815', '0.25317', '-0.14032', '0.0075435', '-0.082932', '-0.09254', '-0.40893', '0.30927', '0.21751', '0.26585', '0.098622', '0.12569', '-0.092163', '-0.22007', '0.22404', '0.12774', '-0.47525', '-0.076955', '0.038016', '0.032717', '-0.020418', '-0.13611', '-0.13123', '0.38105', '-0.20649', '0.088231', '-0.23918', '-0.22516', '-0.20772', '-0.016647', '-0.21131', '-0.075841', '0.12841', '-0.10385', '-0.072705', '0.069345', '-0.25644', '0.49495', '0.45149', '-0.15359', '-0.17156', '0.37173', '-0.06808', '-0.12054', '-0.2553', '-0.26313', '0.096536', '0.34145', '-0.49726', '0.16791', '-0.17355', '0.097742', '0.15472', '-0.23752', '-0.08737', '0.00014656', '0.058978', '0.039838', '0.025135', '-0.070301', '-0.20356', '0.33123', '0.084232', '0.73713', '0.10398', '0.34335', '0.37229', '0.29174', '0.014791', '0.08103', '-0.30984', '-0.16005', '-0.026681', '0.03391', '0.20675', '0.1819', '0.11326', '0.26517', '0.27471', '0.21071', '0.027077', '0.23363', '0.32506', '0.071038', '0.088235', '-0.34817', '0.14361', '-0.45806', '-0.19697', '-0.59363', '-0.031579', '-0.23136', '-0.33682', '0.13242', '-0.044807', '-0.067776', '0.22596', '-0.41027', '-0.11071', '-0.053548', '-0.36116', '-0.018268', '-0.032251', '0.19706', '-0.19434', '0.13703', '-0.35084', '-0.21311', '0.50908', '-0.13747', '-0.017282', '-0.21939', '0.28026', '0.076306', '-0.27734', '-0.21262', '0.24825', '0.18891', '-0.2371', '0.37941', '0.38731', '0.061773', '-0.031349', '0.11106', '0.09931', '0.14731', '-0.11277', '-0.017795', '-0.091664', '-0.064558', '-0.19004', '-0.082415', '-0.023187', '-0.058522', '-0.074836', '-0.20367', '0.44858', '-0.14255', '-0.11196', '0.11035', '-0.053151', '-0.20112', '-0.20838', '-0.085951', '-0.070699', '-0.39669', '0.29692', '-0.30792', '0.16002', '0.33027', '-0.14824', '-0.23991', '0.048473', '-0.19541', '-0.49869', '0.12322', '-0.12284', '0.17234', '0.02402', '0.31135', '-0.057606', '-0.2531', '-0.3098', '-0.22199', '0.015231', '-0.024339', '0.13076', '-0.048296', '0.17392', '0.38798', '0.27862', '-']\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained word-embedding vectors\n",
    "max_words = params['n_words']\n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open(LMDATA/'wiki.id.300K.vec', encoding='utf8')):\n",
    "  if i%50000 == 0:\n",
    "    print(i)\n",
    "  values = line.split()\n",
    "  try:\n",
    "    embeddings_index[\" \".join(values[0:-300])] = numpy.asarray(values[-300:], dtype='float32')\n",
    "  except ValueError:\n",
    "    print(\"Values: {}: {}\".format(i, values))\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(train_df['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), \n",
    "                                     maxlen=params['max_len'])\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), \n",
    "                                     maxlen=params['max_len'])\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if i>=max_words:\n",
    "        break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/mldata/data/LM/id/dataset\n"
     ]
    }
   ],
   "source": [
    "print(LMDATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkNjxT1SEKZG"
   },
   "source": [
    "## Conventional Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFNBPZq535al"
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, \n",
    "                is_neural_net=False, epochs=1):\n",
    "    # fit the training dataset on the classifier\n",
    "    if is_neural_net:\n",
    "      classifier.fit(feature_vector_train, label, epochs=epochs)\n",
    "    else:\n",
    "      classifier.fit(feature_vector_train, label)   \n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "      predictions = [int(round(p[0])) for p in predictions]\n",
    "      #predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    print(\" predictions:\", predictions[:20])\n",
    "    print(\"ground truth:\", valid_y[:20])\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEkAoXG_GAVn"
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1264,
     "status": "ok",
     "timestamp": 1539283612723,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "LE4iaE0X6ezd",
    "outputId": "52cbf831-5869-4c3f-c711-5823dda594b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predictions: [0 2 0 2 3 0 0 2 0 1 0 2 2 0 1 1 3 0 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "NB, Count Vectors:  0.9269195189639223\n",
      " predictions: [0 2 0 2 3 0 1 2 0 1 0 2 2 0 1 1 3 0 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "NB, WordLevel TF-IDF:  0.9161887141535615\n",
      " predictions: [0 2 0 2 1 0 1 2 0 1 0 2 2 0 1 0 3 0 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "NB, N-Gram Vectors:  0.7822386679000926\n",
      " predictions: [0 2 0 2 2 0 0 2 2 1 2 2 0 0 1 2 3 2 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "NB, CharLevel Vectors:  0.8432932469935245\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QA6ro7LjGLFj"
   },
   "source": [
    "### Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5632,
     "status": "ok",
     "timestamp": 1539283618489,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "pWRC2U6N63iy",
    "outputId": "401fe193-6e5f-40c7-9ed7-92c12e0d59ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predictions: [0 2 0 2 2 0 0 2 2 1 0 2 2 0 0 1 3 0 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "LR, Count Vectors:  0.9265494912118409\n",
      " predictions: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 1 0 3 0 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "LR, WordLevel TF-IDF:  0.9178538390379278\n",
      " predictions: [0 2 2 2 1 0 1 2 2 1 0 2 2 0 1 0 3 0 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "LR, N-Gram Vectors:  0.8085106382978723\n",
      " predictions: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 1 2 3 0 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "LR, CharLevel Vectors:  0.8888066604995375\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print( \"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ebqjOIqGYfZ"
   },
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46010,
     "status": "ok",
     "timestamp": 1539283664526,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "tcZq7jDH63v9",
    "outputId": "4c507e05-b62a-4f99-8aa9-24149938ccc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predictions: [0 2 2 2 1 0 1 2 2 1 0 2 2 0 1 0 3 0 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "SVM, N-Gram Vectors:  0.7970397779833488\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "# We have to use the option kernel='linear', the default kernel (rbf) doesn't work \n",
    "# properly (thanks to Leksono Nanto for the hint)\n",
    "accuracy = train_model(svm.SVC(kernel='linear'), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rzKTmXIKGdi8"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7720,
     "status": "ok",
     "timestamp": 1539284372622,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "DGuz7Mk7634V",
    "outputId": "e9b94aeb-47b5-4e26-cb28-b282ddbe29c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predictions: [0 2 0 2 1 0 0 2 2 1 0 2 0 0 0 0 3 0 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "RF, Count Vectors:  0.8392229417206291\n",
      " predictions: [0 2 0 2 1 0 0 2 0 2 0 2 2 0 1 0 3 0 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "RF, WordLevel TF-IDF:  0.8296022201665125\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8h3s81UGkIB"
   },
   "source": [
    "###  Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 180049,
     "status": "ok",
     "timestamp": 1539283853686,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "EXX0OcYi63-f",
    "outputId": "26749603-6643-42b8-f43d-f9030a336f5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predictions: [0 2 2 2 2 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "Xgb, Count Vectors:  0.808695652173913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predictions: [0 2 2 2 2 0 0 2 2 1 0 2 2 0 0 2 3 2 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "Xgb, WordLevel TF-IDF:  0.8070305272895467\n",
      " predictions: [0 2 0 2 2 0 1 2 2 2 0 2 2 0 1 2 3 2 1 3]\n",
      "ground truth: [0 2 0 2 3 0 0 2 2 1 0 2 2 0 0 2 3 0 1 3]\n",
      "Xgb, CharLevel Vectors:  0.8201665124884366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extreme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print(\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extreme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extreme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print(\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KenKn_bDw4k"
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2BE61kS1cYi"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "\n",
    "def tokenize(texts, n_words=1000):\n",
    "    tokenizer = Tokenizer(num_words=n_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "  \n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, texts, labels, tokenizer, batch_size=32, max_len=100,\n",
    "                 n_classes=2, n_words=1000, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.steps_per_epoch = int(np.floor(self.texts.size / self.batch_size))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return self.steps_per_epoch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        texts = np.array([self.texts[k] for k in indexes])\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        X = pad_sequences(sequences, maxlen=self.max_len)\n",
    "        y = np.array([to_categorical(self.labels[k], 4) for k in indexes])\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.texts.size)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOTyMac-ScU-"
   },
   "outputs": [],
   "source": [
    "# Create data generator\n",
    "training_generator = DataGenerator(train_x.values, train_y, token, **params)\n",
    "valid_generator = DataGenerator(valid_x.values, valid_y, token,  **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-ZDXV--SySi"
   },
   "outputs": [],
   "source": [
    "def tpu_wrapper(func):\n",
    "  if TPU_WORKER is not None:\n",
    "    tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
    "    return tpu_model\n",
    "  else:\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRkl0XIh7CZo"
   },
   "outputs": [],
   "source": [
    "# This model doesn't work with current dataset\n",
    "def create_simple_model(input_length=100):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_length, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\", name=\"D1\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(4, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "#classifier = create_simple_model(xtrain_tfidf_ngram.shape[1])\n",
    "#accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, \n",
    "#                       is_neural_net=True, epochs=1)\n",
    "#print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UI6s_qBsSjzb"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#NN, Ngram Level TF IDF Vectors\n",
    "tf.keras.backend.clear_session()\n",
    "classifier = create_simple_model(input_length=params['max_len'])\n",
    "classifier = tpu_wrapper(classifier)\n",
    "classifier.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=valid_generator,\n",
    "    #use_multiprocessing=True,\n",
    "    #workers=6,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1539264998448,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "R-3Rizvqf9N1",
    "outputId": "3e6788c1-ee70-4e55-9081-43c99d39a967"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16214, 5000)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_tfidf_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReP47N1IJ2qn"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def to_tpu(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "      print(\"TPU Wrapper start\")\n",
    "      print(func)\n",
    "      if TPU_WORKER is not None:\n",
    "        tpu_model = tf.contrib.tpu.keras_to_tpu_model(func, strategy)\n",
    "        print(\"TPU exist\")\n",
    "        return tpu_model(*args, **kwargs)\n",
    "      else:\n",
    "        print(\"TPU not exist\")\n",
    "        return func(*args, **kwargs)\n",
    "    print(\"TO_TPU\")\n",
    "    return wrapper\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuHG89VmFdp5"
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRB-JGti7CfX"
   },
   "outputs": [],
   "source": [
    "def create_cnn(input_length=100):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_length, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, \n",
    "                                       weights=[embedding_matrix], \n",
    "                                       trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(100, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.3)(output_layer1)\n",
    "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45825,
     "status": "ok",
     "timestamp": 1539284600530,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "9E2diw6edp0Y",
    "outputId": "bd7f5767-c741-45bd-bf1c-988435c77c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 3s 181ms/step - loss: 1.1163 - acc: 0.5515 - val_loss: 0.7069 - val_acc: 0.8244\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.5830 - acc: 0.8132 - val_loss: 0.3897 - val_acc: 0.8627\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.3882 - acc: 0.8672 - val_loss: 0.3164 - val_acc: 0.8787\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.3269 - acc: 0.8882 - val_loss: 0.2863 - val_acc: 0.8924\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.2988 - acc: 0.8973 - val_loss: 0.2638 - val_acc: 0.8980\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 0.2731 - acc: 0.9061 - val_loss: 0.2566 - val_acc: 0.9021\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 0.2527 - acc: 0.9145 - val_loss: 0.2430 - val_acc: 0.9088\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 0.2426 - acc: 0.9132 - val_loss: 0.2328 - val_acc: 0.9123\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.2300 - acc: 0.9213 - val_loss: 0.2281 - val_acc: 0.9139\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 0.2151 - acc: 0.9255 - val_loss: 0.2235 - val_acc: 0.9158\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 0.1984 - acc: 0.9282 - val_loss: 0.2203 - val_acc: 0.9203\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.1963 - acc: 0.9329 - val_loss: 0.2147 - val_acc: 0.9197\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 0.1836 - acc: 0.9367 - val_loss: 0.2159 - val_acc: 0.9211\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 1s 68ms/step - loss: 0.1733 - acc: 0.9391 - val_loss: 0.2076 - val_acc: 0.9242\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 0.1620 - acc: 0.9433 - val_loss: 0.2077 - val_acc: 0.9250\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 0.1542 - acc: 0.9472 - val_loss: 0.2101 - val_acc: 0.9250\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.1457 - acc: 0.9499 - val_loss: 0.2030 - val_acc: 0.9271\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.1383 - acc: 0.9534 - val_loss: 0.2075 - val_acc: 0.9270\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 1s 85ms/step - loss: 0.1270 - acc: 0.9568 - val_loss: 0.2084 - val_acc: 0.9281\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 0.1235 - acc: 0.9570 - val_loss: 0.2076 - val_acc: 0.9273\n",
      "CPU times: user 27.4 s, sys: 2.78 s, total: 30.1 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "classifier = create_cnn(input_length=params['max_len'])\n",
    "classifier = tpu_wrapper(classifier)\n",
    "classifier.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=valid_generator,\n",
    "    #use_multiprocessing=True,\n",
    "    #workers=6,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "classifier.save_weights(str(LMDATA/'bard.h5'), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1398,
     "status": "ok",
     "timestamp": 1539244721807,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -120
    },
    "id": "iAQacrW3B3Qd",
    "outputId": "0afb27ed-e6b8-4efb-dc50-5ec6271a972a"
   },
   "outputs": [],
   "source": [
    "\n",
    "classifier.save_weights(str(LMDATA/'cnn.h5'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test dataset\n",
    "test_x = test_df['text'].values\n",
    "sequences = token.texts_to_sequences(test_x)\n",
    "test_x_seq = pad_sequences(sequences, maxlen=params['max_len'])\n",
    "#print(valid_x_seq[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2402/2402 [==============================] - 0s 43us/step\n",
      "Loss for final step: 0.21042589038039722, accuracy: 0.9263114071606994\n"
     ]
    }
   ],
   "source": [
    "# predict the labels on test dataset\n",
    "\n",
    "classifier = create_cnn(input_length=params['max_len'])\n",
    "classifier.load_weights(str(LMDATA/'bard.h5'))\n",
    "# TPU can be enabled here if we need  \n",
    "# classifier = tpu_wrapper(classifier)\n",
    "labels = np.array([list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values])\n",
    "score = classifier.evaluate(test_x_seq, labels, verbose=1)\n",
    "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "diSp_8Sg9rrc"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Just for testing\n",
    "labels = [list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values]\n",
    "#print(labels[:5])\n",
    "print(np.array(labels)[:5])\n",
    "print(test_df['label'].values)\n",
    "print(to_categorical(3, 4))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1539284655318,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "ExtDobFzkPmw",
    "outputId": "ac7e333b-e9b6-4a75-8982-e7db2c43f6ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2402/2402 [==============================] - 0s 33us/step\n",
      "Predictions for final step: [0 0 2 3 1 0 0 1 2 2]\n",
      "['Paradoksnya di sisi lain, sinyal akan diakuinya keberadaan lembaga keuangan mikro non formal juga membuat kegelisahan bagi pelakunya.'\n",
      " 'Menurut dia, harga minyak mentah dunia saat ini berada di posisi 126 dolar AS per barel sedikit melemah dibanding hari sebelumnya yang mencapai 127 dolar AS lebih.'\n",
      " 'Pengkajian ini memberikan konfirmasi bahwa kita menghadapi sejenis materi yang berbeda sama sekali, tak seperti yang kita bayangkan.'\n",
      " 'Gol akhir Marco Borriello membuat Genoa mendapat satu angka setelah striker asal Honduras, David Suazo, mencetak angka pada menit ke-12 untuk Inter, yang pemain tengahnya dari Portugal, Pele, dikeluarkan dari lapangan karena mendapat kartu kuning kedua sebelum turun minum.'\n",
      " 'Namun, bandar udara itu ditutup sebagai langkah pencegahan.'\n",
      " 'Departemen Keuangan telah menetapkan 16 calon agen penjual obligasi negara ritel ORI pada 2007 melalui SK Dirjen Pengelolaan Utang Depkeu No KEP-07/PU/2007 tertanggal 19 Februari 2007.'\n",
      " 'Kontrak kerja sama yang tekah ditandatangani dengan China mencapai puluhan juta dollar AS.'\n",
      " 'Sebab kejadian tersebut masih dalam penyelidikan, kata Letnan Kolonel Josslyn Aberle di Bagdad.'\n",
      " 'Kisah Di Balik Suatu Komunikasi Singkat.'\n",
      " 'Proses degradasi material organik ini tanpa melibatkan oksigen disebut anaerobik digestion Gas yang dihasilkan sebagian besar berupa metana.']\n"
     ]
    }
   ],
   "source": [
    "prediction = classifier.predict(test_x_seq, verbose=1)\n",
    "#print(prediction)\n",
    "print('Predictions for final step: {}'.format(np.argmax(prediction, axis=1)[:10]))\n",
    "print(test_x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1365,
     "status": "ok",
     "timestamp": 1539266443312,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "70o-IXxgincl",
    "outputId": "d90d8629-1eff-4fcf-cc74-4cabe26d12ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:       32931128     7942452    12817856      139512    12170820    24099684\r\n",
      "Swap:      16777212           0    16777212\r\n"
     ]
    }
   ],
   "source": [
    "!free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZjhWhQdWogw"
   },
   "outputs": [],
   "source": [
    "\n",
    "#print('Accuracy ', score[1])\n",
    "\"\"\"\n",
    "# First, run the seed forward to prime the state of the model.\n",
    "#prediction_model.reset_states()\n",
    "strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
    "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER))\n",
    "prediction_model = tf.contrib.tpu.keras_to_tpu_model(\n",
    "    prediction_model, strategy=strategy)\n",
    "\n",
    "predictions = prediction_model.predict(valid_x_seq)\n",
    "\n",
    "print(\"predictions\", predictions[:20])\n",
    "\n",
    "predictions = tpu_model.predict(valid_x_seq)\n",
    "\n",
    "predictions = [int(round(p[0])) for p in predictions]\n",
    "#predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "print(\"predictions\", predictions[:20])\n",
    "print(\"valid_y\", valid_y[:20])\n",
    "\n",
    "return metrics.accuracy_score(predictions, valid_y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1539266470311,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "CaL1Vz7AU9A7",
    "outputId": "5f212213-4fc3-4501-baa2-4be082e584f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9131995e-01 2.5257450e-03 6.1324406e-03 2.1910730e-05]\n",
      " [9.9915075e-01 4.9755513e-04 3.4053557e-04 1.1170353e-05]\n",
      " [1.5233310e-01 4.2050965e-02 8.0292422e-01 2.6917094e-03]\n",
      " ...\n",
      " [8.2090375e-04 9.9912328e-01 3.7728907e-05 1.8159501e-05]\n",
      " [1.7484943e-04 7.3114820e-02 3.3964191e-03 9.2331380e-01]\n",
      " [1.0749870e-04 3.1766740e-05 9.9985719e-01 3.5256546e-06]]\n",
      "[0 0 2 ... 1 3 2]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)\n",
    "print(np.argmax(prediction, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0gd-JC7Fj2r"
   },
   "source": [
    "### Kim Yoon’s CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLhmuqLZ1jIN"
   },
   "outputs": [],
   "source": [
    "# The following model is similar to \n",
    "# Kim Yoon’s Convolutional Neural Networks for Sentence Classification\n",
    "# (https://arxiv.org/abs/1408.5882)\n",
    "#\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "filter_sizes = (3, 5, 7)\n",
    "num_filters = 100\n",
    "dropout_prob = (0.5, 0.5)\n",
    "hidden_dims = 50\n",
    "\n",
    "def create_cnn_kimyoon(input_length=100):  \n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_length, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, embedding_dim, \n",
    "                                       weights=[embedding_matrix], \n",
    "                                       trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(dropout_prob[0])(embedding_layer)\n",
    "    \n",
    "    conv_array = []\n",
    "    for sz in filter_sizes:\n",
    "        conv = layers.Convolution1D(filters=num_filters,\n",
    "                             kernel_size=sz,\n",
    "                             padding=\"valid\",\n",
    "                             activation=\"relu\",\n",
    "                             strides=1)(embedding_layer)\n",
    "        conv = layers.MaxPooling1D(pool_size=2)(conv)\n",
    "        conv = layers.Flatten()(conv)\n",
    "        conv_array.append(conv)\n",
    "    \n",
    "    layer = layers.Concatenate()(conv_array) if len(conv_array) > 1 else conv_array[0]\n",
    "    \n",
    "    layer = layers.Dropout(dropout_prob[1])(layer)\n",
    "    layer = layers.Dense(hidden_dims, activation=\"relu\")(layer)\n",
    "    output_layer = layers.Dense(4, activation=\"softmax\")(layer)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=optimizers.Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 73838,
     "status": "ok",
     "timestamp": 1539292660007,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "SD7eUpGQ7s0Y",
    "outputId": "fdee2ab9-988f-4517-9b73-74103b8cadfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 3s 195ms/step - loss: 1.0782 - acc: 0.5348 - val_loss: 0.5741 - val_acc: 0.8094\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 2s 106ms/step - loss: 0.5167 - acc: 0.8118 - val_loss: 0.3501 - val_acc: 0.8775\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.4027 - acc: 0.8559 - val_loss: 0.2947 - val_acc: 0.8924\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.3673 - acc: 0.8676 - val_loss: 0.2827 - val_acc: 0.8992\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 2s 124ms/step - loss: 0.3404 - acc: 0.8764 - val_loss: 0.2780 - val_acc: 0.9018\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.3325 - acc: 0.8805 - val_loss: 0.2765 - val_acc: 0.9047\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 0.3114 - acc: 0.8887 - val_loss: 0.2676 - val_acc: 0.9096\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 0.2968 - acc: 0.8932 - val_loss: 0.2647 - val_acc: 0.9113\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.2828 - acc: 0.8980 - val_loss: 0.2550 - val_acc: 0.9133\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.2742 - acc: 0.8998 - val_loss: 0.2477 - val_acc: 0.9129\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 0.2657 - acc: 0.9032 - val_loss: 0.2465 - val_acc: 0.9146\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 2s 109ms/step - loss: 0.2522 - acc: 0.9099 - val_loss: 0.2433 - val_acc: 0.9148\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 0.2417 - acc: 0.9126 - val_loss: 0.2414 - val_acc: 0.9152\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 2s 105ms/step - loss: 0.2261 - acc: 0.9189 - val_loss: 0.2507 - val_acc: 0.9098\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 2s 102ms/step - loss: 0.2155 - acc: 0.9240 - val_loss: 0.2519 - val_acc: 0.9119\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.2050 - acc: 0.9289 - val_loss: 0.2449 - val_acc: 0.9150\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 2s 112ms/step - loss: 0.1951 - acc: 0.9293 - val_loss: 0.2374 - val_acc: 0.9133\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.1851 - acc: 0.9338 - val_loss: 0.2360 - val_acc: 0.9166\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.1810 - acc: 0.9350 - val_loss: 0.2348 - val_acc: 0.9168\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.1691 - acc: 0.9403 - val_loss: 0.2277 - val_acc: 0.9195\n",
      "CPU times: user 34 s, sys: 5.09 s, total: 39.1 s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "classifier = create_cnn_kimyoon(input_length=params['max_len'])\n",
    "classifier = tpu_wrapper(classifier)\n",
    "classifier.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=valid_generator,\n",
    "    #use_multiprocessing=True,\n",
    "    #workers=6,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "classifier.save_weights(str(LMDATA/'cnn_kimyoon.h5'), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2402/2402 [==============================] - 0s 134us/step\n",
      "Loss for final step: 0.2389291900704048, accuracy: 0.9163197335553706\n"
     ]
    }
   ],
   "source": [
    "# predict the labels on test dataset\n",
    "\n",
    "classifier = create_cnn_kimyoon(input_length=params['max_len'])\n",
    "classifier.load_weights(str(LMDATA/'cnn_kimyoon.h5'))\n",
    "# TPU can be enabled here if we need  \n",
    "# classifier = tpu_wrapper(classifier)\n",
    "labels = np.array([list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values])\n",
    "score = classifier.evaluate(test_x_seq, labels, verbose=1)\n",
    "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkwx3dnI-DxE"
   },
   "source": [
    "### RNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCERYjHmY-o6"
   },
   "outputs": [],
   "source": [
    "def create_rnn_lstm(input_length=100):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_length, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "    #lstm_layer = layers.LSTM(100)(lstm_layer)\n",
    "    #lstm_layer = layers.LSTM(100, return_sequences=True)(lstm_layer)\n",
    "    #lstm_layer = layers.TimeDistributed(layers.Dense(100))(lstm_layer)\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(100, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
    "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17256,
     "status": "ok",
     "timestamp": 1539284820835,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "0q7TZkZQaN5c",
    "outputId": "302ab4c1-19a3-40ec-f998-275935cbba5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "15/15 [==============================] - 9s 627ms/step - loss: 1.1598 - acc: 0.5539 - val_loss: 0.6460 - val_acc: 0.8254\n",
      "Epoch 2/2\n",
      "15/15 [==============================] - 6s 422ms/step - loss: 0.4805 - acc: 0.8472 - val_loss: 0.3420 - val_acc: 0.8754\n",
      "CPU times: user 20.4 s, sys: 2.24 s, total: 22.6 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RNN-LSTM, Word Embeddings\n",
    "# Speed comparison between CPU vs TPU\n",
    "# First we test CPU with 2 epochs\n",
    "tf.keras.backend.clear_session()\n",
    "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
    "#classifier = tf.contrib.tpu.keras_to_tpu_model(classifier, strategy=strategy)\n",
    "classifier.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=valid_generator,\n",
    "    #use_multiprocessing=True,\n",
    "    #workers=6,\n",
    "    epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 131775,
     "status": "ok",
     "timestamp": 1539284962090,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "0MuKd3kFMpzv",
    "outputId": "a13af0ba-3544-48e7-af40-617e82e0997b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 5s 335ms/step - loss: 1.1160 - acc: 0.5629 - val_loss: 0.6229 - val_acc: 0.8266\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 0.4854 - acc: 0.8400 - val_loss: 0.3484 - val_acc: 0.8736\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 3s 180ms/step - loss: 0.3461 - acc: 0.8789 - val_loss: 0.2874 - val_acc: 0.8967\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 3s 179ms/step - loss: 0.3064 - acc: 0.8919 - val_loss: 0.2709 - val_acc: 0.9000\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 3s 194ms/step - loss: 0.2830 - acc: 0.8989 - val_loss: 0.2622 - val_acc: 0.9072\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 3s 173ms/step - loss: 0.2703 - acc: 0.9073 - val_loss: 0.2412 - val_acc: 0.9100\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 3s 184ms/step - loss: 0.2637 - acc: 0.9091 - val_loss: 0.2385 - val_acc: 0.9135\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 3s 168ms/step - loss: 0.2503 - acc: 0.9120 - val_loss: 0.2320 - val_acc: 0.9164\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 0.2464 - acc: 0.9132 - val_loss: 0.2294 - val_acc: 0.9172\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 3s 179ms/step - loss: 0.2352 - acc: 0.9166 - val_loss: 0.2301 - val_acc: 0.9176\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 3s 176ms/step - loss: 0.2344 - acc: 0.9187 - val_loss: 0.2288 - val_acc: 0.9182\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 3s 179ms/step - loss: 0.2222 - acc: 0.9224 - val_loss: 0.2204 - val_acc: 0.9213\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 2s 161ms/step - loss: 0.2171 - acc: 0.9236 - val_loss: 0.2317 - val_acc: 0.9180\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 3s 182ms/step - loss: 0.2159 - acc: 0.9257 - val_loss: 0.2174 - val_acc: 0.9229\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 3s 179ms/step - loss: 0.2097 - acc: 0.9256 - val_loss: 0.2163 - val_acc: 0.9234\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 3s 177ms/step - loss: 0.2011 - acc: 0.9310 - val_loss: 0.2148 - val_acc: 0.9234\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 3s 191ms/step - loss: 0.2004 - acc: 0.9304 - val_loss: 0.2163 - val_acc: 0.9234\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 3s 183ms/step - loss: 0.2009 - acc: 0.9290 - val_loss: 0.2085 - val_acc: 0.9266\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 3s 170ms/step - loss: 0.1824 - acc: 0.9351 - val_loss: 0.2129 - val_acc: 0.9264\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 3s 170ms/step - loss: 0.1828 - acc: 0.9340 - val_loss: 0.2040 - val_acc: 0.9260\n",
      "CPU times: user 1min 46s, sys: 6.95 s, total: 1min 53s\n",
      "Wall time: 56.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RNN-LSTM, Word Embeddings\n",
    "# We test now the TPU\n",
    "# The result is:\n",
    "# CPU: 65s/epoch\n",
    "# TPU: 3.15s/epoch\n",
    "tf.keras.backend.clear_session()\n",
    "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
    "classifier = tpu_wrapper(classifier)\n",
    "classifier.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=valid_generator,\n",
    "    #use_multiprocessing=True,\n",
    "    #workers=6,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "classifier.save_weights(str(LMDATA/'rnn_lstm.h5'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2402/2402 [==============================] - 3s 1ms/step\n",
      "Loss for final step: 0.21542491647821976, accuracy: 0.9304746044962531\n"
     ]
    }
   ],
   "source": [
    "# predict the labels on test dataset\n",
    "\n",
    "classifier = create_rnn_lstm(input_length=params['max_len'])\n",
    "classifier.load_weights(str(LMDATA/'rnn_lstm.h5'))\n",
    "# TPU can be enabled here if we need  \n",
    "# classifier = tpu_wrapper(classifier)\n",
    "labels = np.array([list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values])\n",
    "score = classifier.evaluate(test_x_seq, labels, verbose=1)\n",
    "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDuZt62jE4WH"
   },
   "source": [
    "### RNN-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUaVdVIa7Cqc"
   },
   "outputs": [],
   "source": [
    "def create_rnn_gru(input_length=100):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_length, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 112378,
     "status": "ok",
     "timestamp": 1539285076033,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "NP0Ea8yQNprO",
    "outputId": "8c4c225e-9490-4d65-8daa-56733e6959f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 4s 293ms/step - loss: 1.2397 - acc: 0.4535 - val_loss: 0.9671 - val_acc: 0.7104\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 2s 161ms/step - loss: 0.7800 - acc: 0.7255 - val_loss: 0.4644 - val_acc: 0.8352\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 2s 163ms/step - loss: 0.4623 - acc: 0.8396 - val_loss: 0.3122 - val_acc: 0.8900\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.3488 - acc: 0.8810 - val_loss: 0.2658 - val_acc: 0.9055\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.3132 - acc: 0.8912 - val_loss: 0.2542 - val_acc: 0.9072\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 2s 149ms/step - loss: 0.2905 - acc: 0.8998 - val_loss: 0.2436 - val_acc: 0.9121\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 2s 153ms/step - loss: 0.2754 - acc: 0.9070 - val_loss: 0.2349 - val_acc: 0.9156\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 2s 160ms/step - loss: 0.2605 - acc: 0.9113 - val_loss: 0.2315 - val_acc: 0.9146\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 2s 157ms/step - loss: 0.2527 - acc: 0.9117 - val_loss: 0.2239 - val_acc: 0.9184\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 2s 155ms/step - loss: 0.2438 - acc: 0.9162 - val_loss: 0.2223 - val_acc: 0.9203\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 2s 154ms/step - loss: 0.2425 - acc: 0.9171 - val_loss: 0.2249 - val_acc: 0.9168\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 2s 152ms/step - loss: 0.2319 - acc: 0.9201 - val_loss: 0.2146 - val_acc: 0.9230\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 2s 147ms/step - loss: 0.2258 - acc: 0.9217 - val_loss: 0.2116 - val_acc: 0.9229\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 2s 163ms/step - loss: 0.2208 - acc: 0.9253 - val_loss: 0.2071 - val_acc: 0.9270\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 2s 150ms/step - loss: 0.2215 - acc: 0.9239 - val_loss: 0.2061 - val_acc: 0.9258\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 2s 158ms/step - loss: 0.2157 - acc: 0.9262 - val_loss: 0.2023 - val_acc: 0.9279\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 2s 161ms/step - loss: 0.2046 - acc: 0.9309 - val_loss: 0.2079 - val_acc: 0.9254\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 2s 163ms/step - loss: 0.2011 - acc: 0.9304 - val_loss: 0.2018 - val_acc: 0.9305\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 2s 138ms/step - loss: 0.1942 - acc: 0.9326 - val_loss: 0.1996 - val_acc: 0.9279\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 2s 165ms/step - loss: 0.1930 - acc: 0.9333 - val_loss: 0.1985 - val_acc: 0.9297\n",
      "CPU times: user 1min 33s, sys: 6.27 s, total: 1min 39s\n",
      "Wall time: 49.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RNN-GRU, Word Embeddings\n",
    "tf.keras.backend.clear_session()\n",
    "classifier = create_rnn_gru(input_length=params['max_len'])\n",
    "classifier = tpu_wrapper(classifier)\n",
    "classifier.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=valid_generator,\n",
    "    #use_multiprocessing=True,\n",
    "    #workers=6,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "classifier.save_weights(str(LMDATA/'rnn_gru.h5'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2402/2402 [==============================] - 3s 1ms/step\n",
      "Loss for final step: 0.21358667354996655, accuracy: 0.9296419650291424\n"
     ]
    }
   ],
   "source": [
    "# predict the labels on test dataset\n",
    "\n",
    "classifier = create_rnn_gru(input_length=params['max_len'])\n",
    "classifier.load_weights(str(LMDATA/'rnn_gru.h5'))\n",
    "# TPU can be enabled here if we need  \n",
    "# classifier = tpu_wrapper(classifier)\n",
    "labels = np.array([list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values])\n",
    "score = classifier.evaluate(test_x_seq, labels, verbose=1)\n",
    "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JGxJGMV4E-kV"
   },
   "source": [
    "### Biderectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OzPoMIX07Cyi"
   },
   "outputs": [],
   "source": [
    "# RNN-Bidirectional, Word Embeddings\n",
    "# It doesn't work with TPU, but it works on CPU/GPU\n",
    "def create_bidirectional_rnn(input_length=100):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_length, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(4, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1539287805500,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "yyS7YPyWOiD4",
    "outputId": "b78a6c6d-b15e-4080-ca5e-36b9fd854f16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 7s 441ms/step - loss: 1.2535 - acc: 0.4571 - val_loss: 1.0099 - val_acc: 0.7057\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 4s 251ms/step - loss: 0.8044 - acc: 0.7211 - val_loss: 0.4653 - val_acc: 0.8398\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 4s 252ms/step - loss: 0.4386 - acc: 0.8479 - val_loss: 0.3024 - val_acc: 0.8867\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 4s 252ms/step - loss: 0.3350 - acc: 0.8865 - val_loss: 0.2640 - val_acc: 0.9061\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 4s 249ms/step - loss: 0.3067 - acc: 0.8962 - val_loss: 0.2505 - val_acc: 0.9070\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 4s 245ms/step - loss: 0.2889 - acc: 0.9011 - val_loss: 0.2485 - val_acc: 0.9104\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 4s 247ms/step - loss: 0.2768 - acc: 0.9068 - val_loss: 0.2317 - val_acc: 0.9154\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 4s 254ms/step - loss: 0.2633 - acc: 0.9096 - val_loss: 0.2282 - val_acc: 0.9164\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 4s 246ms/step - loss: 0.2467 - acc: 0.9143 - val_loss: 0.2221 - val_acc: 0.9193\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 4s 249ms/step - loss: 0.2461 - acc: 0.9151 - val_loss: 0.2182 - val_acc: 0.9187\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 4s 255ms/step - loss: 0.2379 - acc: 0.9171 - val_loss: 0.2192 - val_acc: 0.9223\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 4s 243ms/step - loss: 0.2356 - acc: 0.9167 - val_loss: 0.2165 - val_acc: 0.9205\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 4s 249ms/step - loss: 0.2270 - acc: 0.9199 - val_loss: 0.2129 - val_acc: 0.9244\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 4s 240ms/step - loss: 0.2252 - acc: 0.9197 - val_loss: 0.2213 - val_acc: 0.9199\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 4s 245ms/step - loss: 0.2179 - acc: 0.9227 - val_loss: 0.2108 - val_acc: 0.9238\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 4s 257ms/step - loss: 0.2115 - acc: 0.9271 - val_loss: 0.2159 - val_acc: 0.9211\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 4s 249ms/step - loss: 0.2015 - acc: 0.9304 - val_loss: 0.2046 - val_acc: 0.9248\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 4s 245ms/step - loss: 0.1914 - acc: 0.9351 - val_loss: 0.2021 - val_acc: 0.9262\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 4s 244ms/step - loss: 0.1985 - acc: 0.9308 - val_loss: 0.2078 - val_acc: 0.9260\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 4s 249ms/step - loss: 0.1939 - acc: 0.9335 - val_loss: 0.1975 - val_acc: 0.9254\n",
      "CPU times: user 3min 1s, sys: 14.4 s, total: 3min 15s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RNN-Bidirectional, Word Embeddings\n",
    "# It doesn't work with TPU, but it works on CPU/GPU\n",
    "tf.keras.backend.clear_session()\n",
    "classifier = create_bidirectional_rnn(input_length=params['max_len'])\n",
    "#classifier = tpu_wrapper(classifier)\n",
    "classifier.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=valid_generator,\n",
    "    #use_multiprocessing=True,\n",
    "    #workers=6,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "classifier.save_weights(str(LMDATA/'rnn_bidirectional.h5'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2402/2402 [==============================] - 6s 2ms/step\n",
      "Loss for final step: 0.21609353537761997, accuracy: 0.9267277268942548\n"
     ]
    }
   ],
   "source": [
    "# predict the labels on test dataset\n",
    "\n",
    "classifier = create_bidirectional_rnn(input_length=params['max_len'])\n",
    "classifier.load_weights(str(LMDATA/'rnn_bidirectional.h5'))\n",
    "# TPU can be enabled here if we need  \n",
    "# classifier = tpu_wrapper(classifier)\n",
    "labels = np.array([list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values])\n",
    "score = classifier.evaluate(test_x_seq, labels, verbose=1)\n",
    "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PCNhpBuYFOLI"
   },
   "source": [
    "### RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W8lnbJu7owp"
   },
   "outputs": [],
   "source": [
    "def create_rcnn(input_length=100):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_length, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(4, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 751
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 302658,
     "status": "ok",
     "timestamp": 1539248470468,
     "user": {
      "displayName": "cahya wirawan",
      "photoUrl": "",
      "userId": "15378014274179626972"
     },
     "user_tz": -120
    },
    "id": "2uyhGXYp7qBd",
    "outputId": "bec35936-82a9-4d42-c8dc-9fbed1435687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 [==============================] - 2s 140ms/step - loss: 1.1753 - acc: 0.5420 - val_loss: 0.8360 - val_acc: 0.8256\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.6756 - acc: 0.7860 - val_loss: 0.4138 - val_acc: 0.8570\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 0.4532 - acc: 0.8506 - val_loss: 0.3276 - val_acc: 0.8809\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.3805 - acc: 0.8725 - val_loss: 0.2921 - val_acc: 0.8916\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.3327 - acc: 0.8868 - val_loss: 0.2740 - val_acc: 0.8988\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.3076 - acc: 0.8978 - val_loss: 0.2640 - val_acc: 0.9047\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 1s 60ms/step - loss: 0.2840 - acc: 0.9053 - val_loss: 0.2483 - val_acc: 0.9088\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 1s 70ms/step - loss: 0.2677 - acc: 0.9114 - val_loss: 0.2335 - val_acc: 0.9156\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 0.2451 - acc: 0.9196 - val_loss: 0.2276 - val_acc: 0.9178\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 1s 56ms/step - loss: 0.2340 - acc: 0.9239 - val_loss: 0.2243 - val_acc: 0.9178\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 0.2218 - acc: 0.9270 - val_loss: 0.2171 - val_acc: 0.9221\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.2119 - acc: 0.9294 - val_loss: 0.2195 - val_acc: 0.9205\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 0.2015 - acc: 0.9320 - val_loss: 0.2073 - val_acc: 0.9242\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.1840 - acc: 0.9365 - val_loss: 0.2053 - val_acc: 0.9246\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.1822 - acc: 0.9386 - val_loss: 0.2025 - val_acc: 0.9273\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 1s 71ms/step - loss: 0.1708 - acc: 0.9433 - val_loss: 0.2001 - val_acc: 0.9277\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 0.1678 - acc: 0.9439 - val_loss: 0.1985 - val_acc: 0.9283\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 1s 69ms/step - loss: 0.1527 - acc: 0.9469 - val_loss: 0.1952 - val_acc: 0.9293\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 0.1555 - acc: 0.9481 - val_loss: 0.1980 - val_acc: 0.9285\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 1s 86ms/step - loss: 0.1392 - acc: 0.9564 - val_loss: 0.1921 - val_acc: 0.9311\n",
      "CPU times: user 27.3 s, sys: 2.7 s, total: 30 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# RCNN, Word Embeddings\n",
    "tf.keras.backend.clear_session()\n",
    "classifier = create_rcnn(input_length=params['max_len'])\n",
    "classifier = tpu_wrapper(classifier)\n",
    "classifier.fit_generator(\n",
    "    generator=training_generator,\n",
    "    validation_data=valid_generator,\n",
    "    #use_multiprocessing=True,\n",
    "    #workers=6,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "classifier.save_weights(str(LMDATA/'rcnn.h5'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2402/2402 [==============================] - 0s 53us/step\n",
      "Loss for final step: 0.21368380979038495, accuracy: 0.9221482098251457\n"
     ]
    }
   ],
   "source": [
    "# predict the labels on test dataset\n",
    "\n",
    "classifier = create_rcnn(input_length=params['max_len'])\n",
    "classifier.load_weights(str(LMDATA/'rcnn.h5'))\n",
    "# TPU can be enabled here if we need  \n",
    "# classifier = tpu_wrapper(classifier)\n",
    "labels = np.array([list(to_categorical(label, 4).astype(int)) for label in test_df['label'].values])\n",
    "score = classifier.evaluate(test_x_seq, labels, verbose=1)\n",
    "print('Loss for final step: {}, accuracy: {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification - BPPT PANL - TPU.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/cahya-wirawan/ML-Collection/blob/master/Text_Classification_BPPT_PANL_TPU.ipynb",
     "timestamp": 1539246696080
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
