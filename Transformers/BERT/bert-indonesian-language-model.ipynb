{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import transformers, torch\n",
    "from transformers import GPT2Config, AutoConfig\n",
    "from transformers import BertTokenizerFast, BertForMaskedLM, pipeline\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_WATCH\"] = \"all\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"BERT - Indonesian\"\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7f64c20fb850>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='5'\n",
    "torch.cuda.device(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/dataset/wiki/id-100/std_test_bert.txt', '/dataset/wiki/id-100/std_train_bert.txt', '/dataset/wiki/id-100/std_valid_bert.txt'] /output/bert-id-100/base-finetune\n"
     ]
    }
   ],
   "source": [
    "model_type=\"bert\"\n",
    "lang_type=\"id-100\"\n",
    "data_dir = f'/dataset/wiki/{lang_type}'\n",
    "paths = [str(x) for x in Path(data_dir).glob(f'std_*_{model_type}.txt')]\n",
    "output_model=f'/output/{model_type}-{lang_type}/base-finetune'\n",
    "print(paths, output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 30s, sys: 1min 54s, total: 18min 25s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize a tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()  # Bert\n",
    "# Customize training\n",
    "\n",
    "# Bert\n",
    "tokenizer.train(files=paths, vocab_size=32_000, min_frequency=2, special_tokens=[\n",
    "    \"[UNK]\",\n",
    "    \"[SEP]\",\n",
    "    \"[PAD]\",\n",
    "    \"[CLS]\",\n",
    "    \"[MASK]\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/output/bert-id-100/base'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/output/bert-id-100/base/vocab.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save(output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer, BertWordPieceTokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    f'{output_model}/vocab.txt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"Kucing ku makan ikan peda.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'kucing', 'ku', 'makan', 'ikan', 'ped', '##a', '.', '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert-Base\n",
    "config = BertConfig(\n",
    "    vocab_size=32_000,\n",
    "    max_position_embeddings=512,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(output_model)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sysadmin/wirawan/miniconda3/envs/transformers/lib/python3.7/site-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(output_model, max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111241472"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.42 s, sys: 1.68 s, total: 9.11 s\n",
      "Wall time: 9.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset, TextDataset\n",
    "\n",
    "dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=f'{data_dir}/std_train_bert.txt',\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/cahya/BERT%20-%20Indonesian\" target=\"_blank\">https://app.wandb.ai/cahya/BERT%20-%20Indonesian</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/cahya/BERT%20-%20Indonesian/runs/orh64989\" target=\"_blank\">https://app.wandb.ai/cahya/BERT%20-%20Indonesian/runs/orh64989</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=outputs_model,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_gpu_train_batch_size=96,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0444b7875f5944778d3befcb5b563a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1318a46ccd024b9faf107e547de9d791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=8480.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.1 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "/sysadmin/wirawan/miniconda3/envs/transformers/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 7.411663683891296, \"learning_rate\": 4.970518867924528e-05, \"epoch\": 0.0589622641509434, \"step\": 500}\n",
      "{\"loss\": 6.871545332908631, \"learning_rate\": 4.941037735849057e-05, \"epoch\": 0.1179245283018868, \"step\": 1000}\n",
      "{\"loss\": 6.7150249700546265, \"learning_rate\": 4.911556603773585e-05, \"epoch\": 0.17688679245283018, \"step\": 1500}\n",
      "{\"loss\": 6.605466014862061, \"learning_rate\": 4.8820754716981134e-05, \"epoch\": 0.2358490566037736, \"step\": 2000}\n",
      "{\"loss\": 6.522736531257629, \"learning_rate\": 4.852594339622642e-05, \"epoch\": 0.294811320754717, \"step\": 2500}\n",
      "{\"loss\": 6.460593127250672, \"learning_rate\": 4.82311320754717e-05, \"epoch\": 0.35377358490566035, \"step\": 3000}\n",
      "{\"loss\": 6.399852411270142, \"learning_rate\": 4.7936320754716986e-05, \"epoch\": 0.41273584905660377, \"step\": 3500}\n",
      "{\"loss\": 6.364503573417664, \"learning_rate\": 4.7641509433962266e-05, \"epoch\": 0.4716981132075472, \"step\": 4000}\n",
      "{\"loss\": 6.314677367210388, \"learning_rate\": 4.734669811320755e-05, \"epoch\": 0.5306603773584906, \"step\": 4500}\n",
      "{\"loss\": 6.286529247283935, \"learning_rate\": 4.705188679245283e-05, \"epoch\": 0.589622641509434, \"step\": 5000}\n",
      "{\"loss\": 6.260358016967773, \"learning_rate\": 4.675707547169811e-05, \"epoch\": 0.6485849056603774, \"step\": 5500}\n",
      "{\"loss\": 6.22429358959198, \"learning_rate\": 4.64622641509434e-05, \"epoch\": 0.7075471698113207, \"step\": 6000}\n",
      "{\"loss\": 6.199570177078247, \"learning_rate\": 4.616745283018868e-05, \"epoch\": 0.7665094339622641, \"step\": 6500}\n",
      "{\"loss\": 6.1818182077407835, \"learning_rate\": 4.5872641509433964e-05, \"epoch\": 0.8254716981132075, \"step\": 7000}\n",
      "{\"loss\": 6.156112866401672, \"learning_rate\": 4.557783018867925e-05, \"epoch\": 0.8844339622641509, \"step\": 7500}\n",
      "{\"loss\": 6.146166770935059, \"learning_rate\": 4.528301886792453e-05, \"epoch\": 0.9433962264150944, \"step\": 8000}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b229ad38dd42fa844455d4b8be600d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=8480.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 6.121740288734436, \"learning_rate\": 4.4988207547169816e-05, \"epoch\": 1.0023584905660377, \"step\": 8500}\n",
      "{\"loss\": 6.097916290283203, \"learning_rate\": 4.4693396226415095e-05, \"epoch\": 1.0613207547169812, \"step\": 9000}\n",
      "{\"loss\": 6.074948431968689, \"learning_rate\": 4.439858490566038e-05, \"epoch\": 1.1202830188679245, \"step\": 9500}\n",
      "{\"loss\": 5.999774028778076, \"learning_rate\": 4.410377358490566e-05, \"epoch\": 1.179245283018868, \"step\": 10000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "{\"loss\": 5.874733050346374, \"learning_rate\": 4.380896226415094e-05, \"epoch\": 1.2382075471698113, \"step\": 10500}\n",
      "{\"loss\": 5.723001289367676, \"learning_rate\": 4.351415094339623e-05, \"epoch\": 1.2971698113207548, \"step\": 11000}\n",
      "{\"loss\": 5.6048536214828495, \"learning_rate\": 4.3219339622641514e-05, \"epoch\": 1.3561320754716981, \"step\": 11500}\n",
      "{\"loss\": 5.4907419862747195, \"learning_rate\": 4.292452830188679e-05, \"epoch\": 1.4150943396226414, \"step\": 12000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "{\"loss\": 5.344366116523743, \"learning_rate\": 4.262971698113208e-05, \"epoch\": 1.474056603773585, \"step\": 12500}\n",
      "{\"loss\": 5.185864289283752, \"learning_rate\": 4.233490566037736e-05, \"epoch\": 1.5330188679245285, \"step\": 13000}\n",
      "{\"loss\": 5.075927506446838, \"learning_rate\": 4.2040094339622645e-05, \"epoch\": 1.5919811320754715, \"step\": 13500}\n",
      "{\"loss\": 4.929691334724426, \"learning_rate\": 4.1745283018867925e-05, \"epoch\": 1.650943396226415, \"step\": 14000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "{\"loss\": 4.757434042930603, \"learning_rate\": 4.145047169811321e-05, \"epoch\": 1.7099056603773586, \"step\": 14500}\n",
      "{\"loss\": 4.5966146268844605, \"learning_rate\": 4.115566037735849e-05, \"epoch\": 1.7688679245283019, \"step\": 15000}\n",
      "{\"loss\": 4.452615927696228, \"learning_rate\": 4.086084905660378e-05, \"epoch\": 1.8278301886792452, \"step\": 15500}\n",
      "{\"loss\": 4.325474759578705, \"learning_rate\": 4.0566037735849064e-05, \"epoch\": 1.8867924528301887, \"step\": 16000}\n",
      "{\"loss\": 4.236982495307922, \"learning_rate\": 4.027122641509434e-05, \"epoch\": 1.9457547169811322, \"step\": 16500}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c964171aec43b2a41856347d2d3dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=8480.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 4.1530061550140385, \"learning_rate\": 3.997641509433962e-05, \"epoch\": 2.0047169811320753, \"step\": 17000}\n",
      "{\"loss\": 4.071065832614899, \"learning_rate\": 3.968160377358491e-05, \"epoch\": 2.063679245283019, \"step\": 17500}\n",
      "{\"loss\": 3.991127878189087, \"learning_rate\": 3.938679245283019e-05, \"epoch\": 2.1226415094339623, \"step\": 18000}\n",
      "{\"loss\": 3.9261935968399047, \"learning_rate\": 3.9091981132075475e-05, \"epoch\": 2.1816037735849054, \"step\": 18500}\n",
      "{\"loss\": 3.8674808773994447, \"learning_rate\": 3.8797169811320754e-05, \"epoch\": 2.240566037735849, \"step\": 19000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "{\"loss\": 3.8098981595039367, \"learning_rate\": 3.8502358490566034e-05, \"epoch\": 2.2995283018867925, \"step\": 19500}\n",
      "{\"loss\": 3.766021531581879, \"learning_rate\": 3.820754716981133e-05, \"epoch\": 2.358490566037736, \"step\": 20000}\n",
      "{\"loss\": 3.7256044545173643, \"learning_rate\": 3.791273584905661e-05, \"epoch\": 2.417452830188679, \"step\": 20500}\n",
      "{\"loss\": 3.6775840039253236, \"learning_rate\": 3.761792452830189e-05, \"epoch\": 2.4764150943396226, \"step\": 21000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "{\"loss\": 3.6482102704048156, \"learning_rate\": 3.732311320754717e-05, \"epoch\": 2.535377358490566, \"step\": 21500}\n",
      "{\"loss\": 3.6065779643058775, \"learning_rate\": 3.702830188679245e-05, \"epoch\": 2.5943396226415096, \"step\": 22000}\n",
      "{\"loss\": 3.5755018229484556, \"learning_rate\": 3.673349056603774e-05, \"epoch\": 2.6533018867924527, \"step\": 22500}\n",
      "{\"loss\": 3.532718797683716, \"learning_rate\": 3.643867924528302e-05, \"epoch\": 2.7122641509433962, \"step\": 23000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "{\"loss\": 3.5042129311561583, \"learning_rate\": 3.6143867924528304e-05, \"epoch\": 2.7712264150943398, \"step\": 23500}\n",
      "{\"loss\": 3.4768905382156374, \"learning_rate\": 3.5849056603773584e-05, \"epoch\": 2.830188679245283, \"step\": 24000}\n",
      "{\"loss\": 3.4575364413261416, \"learning_rate\": 3.555424528301887e-05, \"epoch\": 2.8891509433962264, \"step\": 24500}\n",
      "{\"loss\": 3.4332490315437316, \"learning_rate\": 3.525943396226416e-05, \"epoch\": 2.94811320754717, \"step\": 25000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfd2613fd074f2a873a12d86c576e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=8480.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 3.4006250624656675, \"learning_rate\": 3.4964622641509436e-05, \"epoch\": 3.0070754716981134, \"step\": 25500}\n",
      "{\"loss\": 3.3741621017456054, \"learning_rate\": 3.466981132075472e-05, \"epoch\": 3.0660377358490565, \"step\": 26000}\n",
      "{\"loss\": 3.3530526728630066, \"learning_rate\": 3.4375e-05, \"epoch\": 3.125, \"step\": 26500}\n",
      "{\"loss\": 3.3272424349784853, \"learning_rate\": 3.408018867924528e-05, \"epoch\": 3.1839622641509435, \"step\": 27000}\n",
      "{\"loss\": 3.3243484535217287, \"learning_rate\": 3.378537735849057e-05, \"epoch\": 3.2429245283018866, \"step\": 27500}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "{\"loss\": 3.285313486099243, \"learning_rate\": 3.349056603773585e-05, \"epoch\": 3.30188679245283, \"step\": 28000}\n",
      "{\"loss\": 3.2684149346351625, \"learning_rate\": 3.3195754716981134e-05, \"epoch\": 3.3608490566037736, \"step\": 28500}\n",
      "{\"loss\": 3.2586914496421815, \"learning_rate\": 3.290094339622642e-05, \"epoch\": 3.419811320754717, \"step\": 29000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 3.252567026615143, \"learning_rate\": 3.26061320754717e-05, \"epoch\": 3.4787735849056602, \"step\": 29500}\n",
      "{\"loss\": 3.2194142332077025, \"learning_rate\": 3.2311320754716986e-05, \"epoch\": 3.5377358490566038, \"step\": 30000}\n",
      "{\"loss\": 3.214864767074585, \"learning_rate\": 3.2016509433962266e-05, \"epoch\": 3.5966981132075473, \"step\": 30500}\n",
      "{\"loss\": 3.1981080584526063, \"learning_rate\": 3.1721698113207545e-05, \"epoch\": 3.6556603773584904, \"step\": 31000}\n",
      "{\"loss\": 3.1853837575912474, \"learning_rate\": 3.142688679245283e-05, \"epoch\": 3.714622641509434, \"step\": 31500}\n",
      "{\"loss\": 3.1670316128730773, \"learning_rate\": 3.113207547169811e-05, \"epoch\": 3.7735849056603774, \"step\": 32000}\n",
      "{\"loss\": 3.1469914193153383, \"learning_rate\": 3.08372641509434e-05, \"epoch\": 3.8325471698113205, \"step\": 32500}\n",
      "{\"loss\": 3.1435256175994875, \"learning_rate\": 3.054245283018868e-05, \"epoch\": 3.891509433962264, \"step\": 33000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "{\"loss\": 3.143414912223816, \"learning_rate\": 3.0247641509433967e-05, \"epoch\": 3.9504716981132075, \"step\": 33500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de43f88d04264d3ea10e502beef55953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=8480.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 3.1265247435569763, \"learning_rate\": 2.995283018867925e-05, \"epoch\": 4.009433962264151, \"step\": 34000}\n",
      "{\"loss\": 3.110561480522156, \"learning_rate\": 2.965801886792453e-05, \"epoch\": 4.068396226415095, \"step\": 34500}\n",
      "{\"loss\": 3.093845167160034, \"learning_rate\": 2.9363207547169812e-05, \"epoch\": 4.127358490566038, \"step\": 35000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 3.0941307692527773, \"learning_rate\": 2.9068396226415095e-05, \"epoch\": 4.186320754716981, \"step\": 35500}\n",
      "{\"loss\": 3.071337870121002, \"learning_rate\": 2.8773584905660378e-05, \"epoch\": 4.245283018867925, \"step\": 36000}\n",
      "{\"loss\": 3.0709372782707214, \"learning_rate\": 2.847877358490566e-05, \"epoch\": 4.304245283018868, \"step\": 36500}\n",
      "{\"loss\": 3.051677589416504, \"learning_rate\": 2.8183962264150944e-05, \"epoch\": 4.363207547169811, \"step\": 37000}\n",
      "{\"loss\": 3.0497585949897767, \"learning_rate\": 2.7889150943396224e-05, \"epoch\": 4.422169811320755, \"step\": 37500}\n",
      "{\"loss\": 3.0406925077438354, \"learning_rate\": 2.7594339622641513e-05, \"epoch\": 4.481132075471698, \"step\": 38000}\n",
      "{\"loss\": 3.0310194072723387, \"learning_rate\": 2.7299528301886796e-05, \"epoch\": 4.540094339622642, \"step\": 38500}\n",
      "{\"loss\": 3.022010278224945, \"learning_rate\": 2.700471698113208e-05, \"epoch\": 4.599056603773585, \"step\": 39000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "{\"loss\": 3.0154786038398744, \"learning_rate\": 2.670990566037736e-05, \"epoch\": 4.658018867924528, \"step\": 39500}\n",
      "{\"loss\": 3.012351972579956, \"learning_rate\": 2.641509433962264e-05, \"epoch\": 4.716981132075472, \"step\": 40000}\n",
      "{\"loss\": 2.996200574874878, \"learning_rate\": 2.6120283018867925e-05, \"epoch\": 4.775943396226415, \"step\": 40500}\n",
      "{\"loss\": 2.984955916404724, \"learning_rate\": 2.5825471698113208e-05, \"epoch\": 4.834905660377358, \"step\": 41000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "{\"loss\": 2.9768201642036436, \"learning_rate\": 2.553066037735849e-05, \"epoch\": 4.893867924528302, \"step\": 41500}\n",
      "{\"loss\": 2.9723486037254334, \"learning_rate\": 2.5235849056603777e-05, \"epoch\": 4.952830188679245, \"step\": 42000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010a7b471c6d4a08b861a0cc6ee61d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=8480.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.9803603062629698, \"learning_rate\": 2.4941037735849056e-05, \"epoch\": 5.011792452830188, \"step\": 42500}\n",
      "{\"loss\": 2.9559700865745544, \"learning_rate\": 2.464622641509434e-05, \"epoch\": 5.070754716981132, \"step\": 43000}\n",
      "{\"loss\": 2.940078497886658, \"learning_rate\": 2.4351415094339626e-05, \"epoch\": 5.129716981132075, \"step\": 43500}\n",
      "{\"loss\": 2.937126225948334, \"learning_rate\": 2.405660377358491e-05, \"epoch\": 5.188679245283019, \"step\": 44000}\n",
      "{\"loss\": 2.9336873950958253, \"learning_rate\": 2.3761792452830188e-05, \"epoch\": 5.247641509433962, \"step\": 44500}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.93406600522995, \"learning_rate\": 2.346698113207547e-05, \"epoch\": 5.306603773584905, \"step\": 45000}\n",
      "{\"loss\": 2.9258333911895753, \"learning_rate\": 2.3172169811320758e-05, \"epoch\": 5.365566037735849, \"step\": 45500}\n",
      "{\"loss\": 2.9103901352882384, \"learning_rate\": 2.287735849056604e-05, \"epoch\": 5.4245283018867925, \"step\": 46000}\n",
      "{\"loss\": 2.9154199624061583, \"learning_rate\": 2.258254716981132e-05, \"epoch\": 5.4834905660377355, \"step\": 46500}\n",
      "{\"loss\": 2.9011168608665465, \"learning_rate\": 2.2287735849056603e-05, \"epoch\": 5.5424528301886795, \"step\": 47000}\n",
      "{\"loss\": 2.899539417743683, \"learning_rate\": 2.199292452830189e-05, \"epoch\": 5.601415094339623, \"step\": 47500}\n",
      "{\"loss\": 2.9019356575012205, \"learning_rate\": 2.1698113207547172e-05, \"epoch\": 5.660377358490566, \"step\": 48000}\n",
      "{\"loss\": 2.8844442529678345, \"learning_rate\": 2.1403301886792455e-05, \"epoch\": 5.71933962264151, \"step\": 48500}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.8818497762680053, \"learning_rate\": 2.1108490566037735e-05, \"epoch\": 5.778301886792453, \"step\": 49000}\n",
      "{\"loss\": 2.8881214294433595, \"learning_rate\": 2.0813679245283018e-05, \"epoch\": 5.837264150943396, \"step\": 49500}\n",
      "{\"loss\": 2.882572271347046, \"learning_rate\": 2.0518867924528304e-05, \"epoch\": 5.89622641509434, \"step\": 50000}\n",
      "{\"loss\": 2.869346297740936, \"learning_rate\": 2.0224056603773587e-05, \"epoch\": 5.955188679245283, \"step\": 50500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa452ce1c2134593a9314f7679fb57e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=8480.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.8626720790863036, \"learning_rate\": 1.992924528301887e-05, \"epoch\": 6.014150943396227, \"step\": 51000}\n",
      "{\"loss\": 2.85968260717392, \"learning_rate\": 1.963443396226415e-05, \"epoch\": 6.07311320754717, \"step\": 51500}\n",
      "{\"loss\": 2.8648132448196413, \"learning_rate\": 1.9339622641509436e-05, \"epoch\": 6.132075471698113, \"step\": 52000}\n",
      "{\"loss\": 2.8394255776405335, \"learning_rate\": 1.904481132075472e-05, \"epoch\": 6.191037735849057, \"step\": 52500}\n",
      "{\"loss\": 2.83781196641922, \"learning_rate\": 1.8750000000000002e-05, \"epoch\": 6.25, \"step\": 53000}\n",
      "{\"loss\": 2.8368638310432432, \"learning_rate\": 1.8455188679245285e-05, \"epoch\": 6.308962264150943, \"step\": 53500}\n",
      "{\"loss\": 2.828991418838501, \"learning_rate\": 1.8160377358490564e-05, \"epoch\": 6.367924528301887, \"step\": 54000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.8346480121612547, \"learning_rate\": 1.786556603773585e-05, \"epoch\": 6.42688679245283, \"step\": 54500}\n",
      "{\"loss\": 2.8199359951019285, \"learning_rate\": 1.7570754716981134e-05, \"epoch\": 6.485849056603773, \"step\": 55000}\n",
      "{\"loss\": 2.8281976613998414, \"learning_rate\": 1.7275943396226416e-05, \"epoch\": 6.544811320754717, \"step\": 55500}\n",
      "{\"loss\": 2.807712866306305, \"learning_rate\": 1.69811320754717e-05, \"epoch\": 6.60377358490566, \"step\": 56000}\n",
      "{\"loss\": 2.825312567234039, \"learning_rate\": 1.6686320754716982e-05, \"epoch\": 6.662735849056604, \"step\": 56500}\n",
      "{\"loss\": 2.804416844844818, \"learning_rate\": 1.6391509433962265e-05, \"epoch\": 6.721698113207547, \"step\": 57000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.8015453329086304, \"learning_rate\": 1.6096698113207548e-05, \"epoch\": 6.78066037735849, \"step\": 57500}\n",
      "{\"loss\": 2.809866195678711, \"learning_rate\": 1.580188679245283e-05, \"epoch\": 6.839622641509434, \"step\": 58000}\n",
      "{\"loss\": 2.797716398715973, \"learning_rate\": 1.5507075471698114e-05, \"epoch\": 6.898584905660377, \"step\": 58500}\n",
      "{\"loss\": 2.7990039629936216, \"learning_rate\": 1.5212264150943397e-05, \"epoch\": 6.9575471698113205, \"step\": 59000}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74074b6cf1a4bdaa90f54fb775989dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=8480.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.790600723743439, \"learning_rate\": 1.491745283018868e-05, \"epoch\": 7.0165094339622645, \"step\": 59500}\n",
      "{\"loss\": 2.780982835292816, \"learning_rate\": 1.4622641509433963e-05, \"epoch\": 7.0754716981132075, \"step\": 60000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.785551554679871, \"learning_rate\": 1.4327830188679244e-05, \"epoch\": 7.134433962264151, \"step\": 60500}\n",
      "{\"loss\": 2.778546877861023, \"learning_rate\": 1.403301886792453e-05, \"epoch\": 7.193396226415095, \"step\": 61000}\n",
      "{\"loss\": 2.7700710458755493, \"learning_rate\": 1.3738207547169812e-05, \"epoch\": 7.252358490566038, \"step\": 61500}\n",
      "{\"loss\": 2.7746652050018312, \"learning_rate\": 1.3443396226415095e-05, \"epoch\": 7.311320754716981, \"step\": 62000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.7679100184440615, \"learning_rate\": 1.3148584905660378e-05, \"epoch\": 7.370283018867925, \"step\": 62500}\n",
      "{\"loss\": 2.7679089097976686, \"learning_rate\": 1.2853773584905662e-05, \"epoch\": 7.429245283018868, \"step\": 63000}\n",
      "{\"loss\": 2.7669439082145693, \"learning_rate\": 1.2558962264150945e-05, \"epoch\": 7.488207547169811, \"step\": 63500}\n",
      "{\"loss\": 2.7566031765937806, \"learning_rate\": 1.2264150943396227e-05, \"epoch\": 7.547169811320755, \"step\": 64000}\n",
      "{\"loss\": 2.7628654890060425, \"learning_rate\": 1.196933962264151e-05, \"epoch\": 7.606132075471698, \"step\": 64500}\n",
      "{\"loss\": 2.7491676335334776, \"learning_rate\": 1.1674528301886793e-05, \"epoch\": 7.665094339622642, \"step\": 65000}\n",
      "{\"loss\": 2.751494884967804, \"learning_rate\": 1.1379716981132075e-05, \"epoch\": 7.724056603773585, \"step\": 65500}\n",
      "{\"loss\": 2.743039944648743, \"learning_rate\": 1.108490566037736e-05, \"epoch\": 7.783018867924528, \"step\": 66000}\n",
      "{\"loss\": 2.747424954891205, \"learning_rate\": 1.0790094339622641e-05, \"epoch\": 7.841981132075472, \"step\": 66500}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "{\"loss\": 2.7521715598106384, \"learning_rate\": 1.0495283018867926e-05, \"epoch\": 7.900943396226415, \"step\": 67000}\n",
      "{\"loss\": 2.7424176654815673, \"learning_rate\": 1.0200471698113207e-05, \"epoch\": 7.959905660377358, \"step\": 67500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32784de00f584602b89a466eb4ca1ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=8480.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.7381920380592346, \"learning_rate\": 9.905660377358492e-06, \"epoch\": 8.018867924528301, \"step\": 68000}\n",
      "{\"loss\": 2.739515981197357, \"learning_rate\": 9.610849056603773e-06, \"epoch\": 8.077830188679245, \"step\": 68500}\n",
      "{\"loss\": 2.743640904903412, \"learning_rate\": 9.316037735849056e-06, \"epoch\": 8.13679245283019, \"step\": 69000}\n",
      "{\"loss\": 2.7368316206932066, \"learning_rate\": 9.02122641509434e-06, \"epoch\": 8.195754716981131, \"step\": 69500}\n",
      "{\"loss\": 2.7291755394935606, \"learning_rate\": 8.726415094339622e-06, \"epoch\": 8.254716981132075, \"step\": 70000}\n",
      "{\"loss\": 2.734577302932739, \"learning_rate\": 8.431603773584907e-06, \"epoch\": 8.31367924528302, \"step\": 70500}\n",
      "{\"loss\": 2.7323984031677244, \"learning_rate\": 8.136792452830188e-06, \"epoch\": 8.372641509433961, \"step\": 71000}\n",
      "{\"loss\": 2.725057454586029, \"learning_rate\": 7.841981132075473e-06, \"epoch\": 8.431603773584905, \"step\": 71500}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1048576.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.715803949832916, \"learning_rate\": 7.547169811320755e-06, \"epoch\": 8.49056603773585, \"step\": 72000}\n",
      "{\"loss\": 2.72225368642807, \"learning_rate\": 7.2523584905660384e-06, \"epoch\": 8.549528301886792, \"step\": 72500}\n",
      "{\"loss\": 2.7161327362060548, \"learning_rate\": 6.9575471698113205e-06, \"epoch\": 8.608490566037736, \"step\": 73000}\n",
      "{\"loss\": 2.715893136024475, \"learning_rate\": 6.662735849056604e-06, \"epoch\": 8.66745283018868, \"step\": 73500}\n",
      "{\"loss\": 2.71194309091568, \"learning_rate\": 6.367924528301887e-06, \"epoch\": 8.726415094339622, \"step\": 74000}\n",
      "{\"loss\": 2.7079863052368163, \"learning_rate\": 6.07311320754717e-06, \"epoch\": 8.785377358490566, \"step\": 74500}\n",
      "{\"loss\": 2.7020029015541076, \"learning_rate\": 5.778301886792453e-06, \"epoch\": 8.84433962264151, \"step\": 75000}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.7156139469146727, \"learning_rate\": 5.483490566037736e-06, \"epoch\": 8.903301886792454, \"step\": 75500}\n",
      "{\"loss\": 2.7073698620796205, \"learning_rate\": 5.188679245283019e-06, \"epoch\": 8.962264150943396, \"step\": 76000}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6e48c0745345d9a2ad28a2198b2d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=8480.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.7112299284934998, \"learning_rate\": 4.893867924528302e-06, \"epoch\": 9.02122641509434, \"step\": 76500}\n",
      "{\"loss\": 2.7091905012130737, \"learning_rate\": 4.599056603773585e-06, \"epoch\": 9.080188679245284, \"step\": 77000}\n",
      "{\"loss\": 2.7012497153282164, \"learning_rate\": 4.304245283018868e-06, \"epoch\": 9.139150943396226, \"step\": 77500}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.706629996776581, \"learning_rate\": 4.009433962264151e-06, \"epoch\": 9.19811320754717, \"step\": 78000}\n",
      "{\"loss\": 2.702580201625824, \"learning_rate\": 3.714622641509434e-06, \"epoch\": 9.257075471698114, \"step\": 78500}\n",
      "{\"loss\": 2.6997499742507935, \"learning_rate\": 3.419811320754717e-06, \"epoch\": 9.316037735849056, \"step\": 79000}\n",
      "{\"loss\": 2.689007068157196, \"learning_rate\": 3.125e-06, \"epoch\": 9.375, \"step\": 79500}\n",
      "{\"loss\": 2.707279338359833, \"learning_rate\": 2.830188679245283e-06, \"epoch\": 9.433962264150944, \"step\": 80000}\n",
      "{\"loss\": 2.6919138793945314, \"learning_rate\": 2.5353773584905665e-06, \"epoch\": 9.492924528301886, \"step\": 80500}\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 524288.0\n",
      "{\"loss\": 2.694211133003235, \"learning_rate\": 2.240566037735849e-06, \"epoch\": 9.55188679245283, \"step\": 81000}\n",
      "{\"loss\": 2.694009170055389, \"learning_rate\": 1.945754716981132e-06, \"epoch\": 9.610849056603774, \"step\": 81500}\n",
      "{\"loss\": 2.698002909183502, \"learning_rate\": 1.650943396226415e-06, \"epoch\": 9.669811320754716, \"step\": 82000}\n",
      "{\"loss\": 2.7002882800102235, \"learning_rate\": 1.3561320754716983e-06, \"epoch\": 9.72877358490566, \"step\": 82500}\n",
      "{\"loss\": 2.6890647807121275, \"learning_rate\": 1.0613207547169812e-06, \"epoch\": 9.787735849056604, \"step\": 83000}\n",
      "{\"loss\": 2.702919611930847, \"learning_rate\": 7.665094339622642e-07, \"epoch\": 9.846698113207546, \"step\": 83500}\n",
      "{\"loss\": 2.6853654141426087, \"learning_rate\": 4.7169811320754717e-07, \"epoch\": 9.90566037735849, \"step\": 84000}\n",
      "{\"loss\": 2.6950514097213745, \"learning_rate\": 1.768867924528302e-07, \"epoch\": 9.964622641509434, \"step\": 84500}\n",
      "\n",
      "\n",
      "CPU times: user 5h 22min 32s, sys: 3h 1min 6s, total: 8h 23min 38s\n",
      "Wall time: 4h 52min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=84800, training_loss=3.54714225102708)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(outputs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=output_model,\n",
    "    tokenizer=output_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] ibu ku sedang bekerja di supermarket [SEP]',\n",
       "  'score': 0.7983310222625732,\n",
       "  'token': 1495},\n",
       " {'sequence': '[CLS] ibu ku sedang bekerja. supermarket [SEP]',\n",
       "  'score': 0.090003103017807,\n",
       "  'token': 17},\n",
       " {'sequence': '[CLS] ibu ku sedang bekerja sebagai supermarket [SEP]',\n",
       "  'score': 0.025469014421105385,\n",
       "  'token': 1600},\n",
       " {'sequence': '[CLS] ibu ku sedang bekerja dengan supermarket [SEP]',\n",
       "  'score': 0.017966199666261673,\n",
       "  'token': 1555},\n",
       " {'sequence': '[CLS] ibu ku sedang bekerja untuk supermarket [SEP]',\n",
       "  'score': 0.016971781849861145,\n",
       "  'token': 1572}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"ibu ku sedang bekerja [MASK] supermarket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] ibu ku sedang berada di supermarket [SEP]',\n",
       "  'score': 0.24948164820671082,\n",
       "  'token': 2186},\n",
       " {'sequence': '[CLS] ibu ku sedang bekerja di supermarket [SEP]',\n",
       "  'score': 0.08426331728696823,\n",
       "  'token': 2730},\n",
       " {'sequence': '[CLS] ibu ku sedang ada di supermarket [SEP]',\n",
       "  'score': 0.04405415058135986,\n",
       "  'token': 1821},\n",
       " {'sequence': '[CLS] ibu ku sedang bermain di supermarket [SEP]',\n",
       "  'score': 0.036261286586523056,\n",
       "  'token': 2715},\n",
       " {'sequence': '[CLS] ibu ku sedang tinggal di supermarket [SEP]',\n",
       "  'score': 0.03429163992404938,\n",
       "  'token': 2757}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"ibu ku sedang [MASK] di supermarket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
